- 26-9-2022: created

- "pre-trained model", "[[Self-supervised learning|self-supervised]] model". 



- Adapatation required: 
	- They are intermediary assets; they are unfinished and generally should not be used directly, instead requiring adaptation for specific [[downstream tasks]] [[tasks]]. 

- Modeling qualities: (R1)
	- Next generation of models (from [[On the Opportunities and Risks of Foundation Models (Bommasani et. al, 2021).pdf]])
		- [[Multimodality]] (to consume, process and potentially produce content from different sources and domains, such as text + image)
		- Memory capacity (to effectively store and retrieve the acquired knowledge)
		- Compositionality (to foster successful generalization to novel settings and environments)

- Training qualities: (R1) 
	- Instead of modality-specific objectives, which often heuristically.
	- training objective for foundational models will have 2 changes: 
		- "principled selection", derived from systematic evidence and evaluation.
		- "domain-generality", to provide rich, scalable and unified training signal across data sources and modalities. 
	- Trade-off between [[Generative learning algorithms]] and [[Discriminative learning algorithms]]

---
## Application of foundational models

- [[natural language processing|NLP]]
- [[Reasoning]]




---
## Reference:
1.  [[(Paper) On the Opportunities and Risks of Foundation Models (Bommasani et. al, 2021)]]