---
category: []
alias: []
tags: []
---

- 23-10-2022 14:52: created

- subset:
	- [[softmax]]
	- [[Logistic]]

- What is activation?
	- non-linearity?

Recall [[linearity]] limits functional responses
	- We extend the linear mappings to non linear functions with activation function. This approach is valid based on [[universal approximation theorem]], where $f(\cdot)$ is a specfied activation function. 

- Activation function is assigned during the time of creating a [[neural network]].  (my writing)
- Activation function produces probability of the event that "some labels is true". (my writing)
	- eg: $h_\theta(x) = g(z) = g(\theta^Tx)$, which $g$ is the activation function that accepting a linear regression function $z$ into it. And each linear [[regression]] contains all parameters and input variables.


---
## Reference

1. 