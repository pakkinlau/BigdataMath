- 12-10-2022: created

- This is made by myself

- This model suggests chopping a character sequence OR a defined document unit into pieces, called tokens, perhaps at the same time throwing away certain characters, such as punctuation. Then among smaller units we can easier assign meanings. (R1) 


---
## Reference
1. Stanford NLP group's blog: https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html