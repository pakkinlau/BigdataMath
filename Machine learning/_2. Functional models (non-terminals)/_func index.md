- 25-9-2022: created

---
- What are functions:
	- Calculates or manipulates data, and produce results. 
	- Algorithms
- Abstract
---
### Requirement of each page:
1. Each page should put the abstract (with json like format) on the top, such that in view-mode the reader and hover over the link and grasp 80% ideas of the page. 

---
- The scope of machine learning
	- Paradigm
		- Supervised / unsupervised / Foundational model
	- Process
		- Dataset
			- Images, texts, sequential data, graph
		- Feature / representation learning 
			- Co-occurence --> Vector embedding (the predefined function by the designer)
			- 
		- Forward and backward propagation (That learn the parameters of the neuron of the NN.)
			- Regression
			- Cross entropy


---


### Abstract functions

- Big picture
	- [[machine learning]]

- Basic math
	- priori (What is that?)
	- [[Underset]]
	- [[overset]]
	- [[latent]]
	- [[dot product]]
	- [[category theory]]
	- [[Invariant, shift-invariant, time-invariant]]
	- [[Equal to by definition]]
	- [[argmax (arguments of the maxima)]]
	- [[Multivariate Gaussian]]
	- [[Bernoulli distribution]]
	- [[identical and independently distributed]]
	- [[set theory]]
		- [[union of infinite set]]
		- [[cartesian product]]

---
## Learning 
- ML is made of 1. Optimization and 2. Approximation. 

- Regression
	- [[Maximum likelihood estimation (MLE)
	- [[Generalized linear models]]

- Algorithms
	- Graph algorithms
		- [[Breadth first search]]
		- [[Depth first search]]

- Statistics
	- [[Cross entropy]]

- Processing: 
	- Frobenious norm

- Feature / representation learning (old paradigm: feature engineering (replacing manual feature engineering because it is cumbersome and limits the capability of the machine.))
	- Theories
		- [[Representation theory]]: Mathematical
	- Types 
		- [[Page rank algorithm]] (importance of a node)
		- [[Kernel space]]
		- [[Feature map]]
		- [[Embedding]]
		- [[Receptive field]]

- NN
	- [[neuron]]

- Graphs:
	- [[Adjacency matrix]]
	- Encoder: $f:u \rightarrow R^d : f(u):z_u$,
	- [[Decoder]]

- Grouping and clustering
	- [[Entity grouping algorithm]]

- Learning paradigms:
	- [[Foundational model]]
	- [[Heuristics]]
	- [[Discriminative learning algorithms]]
	- [[Generative learning algorithms]]
		- [[Gaussian discriminant analysis]]
	- [[Conventional methods]]
		- Data preparation as optimization 
		- [[machine learning]]
		- [[Hyperparameter tuning ]]
		- [[Model selection ]]
	- [[Approximation]]


- [[supervised learning]] 
	- [[Loss function]]
	- [[Loss of information]]
	- [[Gradient descent]]

- [[Unsupervised learning]]
	- [[EM (Expactation-maximization) algorithm]]
	- [[Self-supervised learning]]
	- Graphs
		- Random forest
		- [[Support vector machine]]
		- Node similarity optimization
			- [[Log-likelihood objective]]: Analytical complete method, but it is costly in computation. 
			- Random walk

- Sampling
	- Negative sampling

- Global matrix factorization methods
	- LSA
	- HAL
	- COALS
	- Hellinger-PCA
	- [[Latent Dirichlet Allocation]]
	- [[LSI]]
	- [[singular value decomposition]]

- Testing the model
	- [[Generalization]]

- [[category theory|category theory]]
	- [[category theory|category]]
	- [[olog]]
	- [[quiver]]
	- [[directed]]
	- [[Product (category)]]

---
### Concrete functions

- Architecture
	- [[Deep]]
	- MLP
	- GCN
	- [[Convolutional]]
	- Shared-weight arhitecture
	- [[Recurrent]]

- Data preprocessing:
	- Stemming, stop words, punctuation, lowercasing


- [[natural language processing]]: 
	- [[Word representation]]
		- [[One-hot vector]]
		- [[GloVe]]
		- [[Word2vec]]
	- Windows
		- N-gram
		- [[Skip-gram]]
		- [[Local context window methods]]
	- Word statistics
		- [[TF-IDF]]
	- Attention
		- [[Transformer]]: underpins most foundation models to date.

- Reducing computation cost
	- [[Negative sampling]]
	- [[Stochastic gradient descent]]

- Sort out something
	- [[Matrix factorization]]


- [[Reasoning]]
	- [[General space]]
	- [[Heuristics]]



- [[Knowledge graph completion]]
	- [[KGR Performance indicators]]
	- Technologies for KGC:
		- Traditional
		- Represenation learning based 
		- Neural network based model
