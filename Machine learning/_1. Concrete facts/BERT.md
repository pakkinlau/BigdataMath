- 4-10-2022: created

- Bidirectional Encoder Representations from Transformers (BERT) is a [[Transformer]]-based machine learning technique for natural language processing (NLP) pre-training developed by Google.

BERT: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

- More: https://www.wikiwand.com/en/BERT_(language_model)