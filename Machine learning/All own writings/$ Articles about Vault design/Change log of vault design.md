### The following parts still haven't worked out on "vault design" page

- 6/11/2023
	- Counting the usage of one notes, versus the number of content within one note
	- If there are a lot of content in the note, but rarely open or mention the note. We might consider the note is not well-connected. 

- 24/10/2023:
	- Location token:
		- When I create notes for a subject of academic matter, I would follow the sequence of chapters in the book. 
		- Assume we have some notes within collections are index page. 
		- When we represent a collection of notes as a graph, we would put location token on each work token within that index page. 

- 20/9/2023:
	- The design of LLMs also can be represented as linear algebra format. 
		- Such as transformer - Actually adding a "pattern" embedding layer is mimicking matrix decomposition in linear algebra

- 19/9/2023:
	- Research ideas:
		- Develop linear algebra representations to express the complete process of "developing and constructing a large language model from start to end"
		- Mathematical frameworks for effective prompt engineering with large language models 
			- Techniques:
				- Tree-structure prompting / chain-of-thought prompting (the mass of people doing that)
				- Variable prompting 
				- Symbolic reasoning (ask chatGPT converting the questions into a simpler symbolic form)
				- My idea:
					- Symbolic / graphical prompt engineering
						- Discussion on this idea see: `meta` vault, > `registers of sequences` page
			- Related theories 
				- Information retrieval
				- Optimization
				- Probabilistic models
				- Information theory
				- Reinforcement learning 
				- Natural language understanding
				- Representation learning 
				- Evaluation metrics
				- Human feedback and crowdsourcing 
				- Active learning 

- 9/9/2023:
	- Tomorrow task:
		- 1. Get to know the basic functionality of mongoDB
		- 2. Use mongoDB to manipulate the vault:
			- a. lower-casing, alias management (might be generated by hugging face transformer) of the entity
			- b. automatically bracketing `[[]]` if there is any established entities (exclude the entity of the current note) found in the main content of one note. 
		- Doing this package would help me complete the population of vault more effectively. 

- 7/9/2023:
	- 1. ChatGPT output processor
		- Idea: With some kind of prompt, the response most of the time respond in dictionary-like bullet points. But that format is not align with my style.
			- 1. Design a proper format.
			- 2. Write a program what I can freely feed any chatGPT response to the script, and then return an edited response that align my desired data format. 
	- 2. medium.com blog writing idea:
		- 1. Create a sequence, that allow the user can enter a series of variables. Each variable can either carries a paragraph, a sentence, or an entity.

- 6/9/2023:
	- 1. For most entities, we can just simply enforce NLP pre-processing techniques, such as "stemming", "lemmatization", "lower casing", "tokenization", "stop word removal", and then apply these rules to all entities in the vault
		- An alternate idea is, use the processed token as the name of the note, and use all possible variations "before processing" as alias. 
		- Solution: Make two lists, 
			- one is "alias", that allow the user to put in possible alternative names of the same concept. 
			- one is "aliases", that is the output zone of the an exhaustive list of NLP generated possible alternative nouns
		- Task:
			- Step 1 - pre-processing
				- Study the data: count number of word for each entity, put the words in ascending order by the word length. 
				- Make decision: How to decide the rule that "the note" is an entity? Might be more than 3 words --> Not a concept. which might be a book, a course etc. 
			- Step 2 - really work on the algorithm
				- Write a function that serve one notes
				- Loop over all notes and apply that function
	- 2. For some similar concepts, might be sometimes we can just use ![[]] from a more complicated concept, to import simpler concepts, to express "composed by" 


- 5/9/2023:
	- 1. The node size of the graph should be able to change with the content length of that note.
	- 2. We could make a script that do the following:
		- a. Feed all entities into the program
		- b. Suggest categorization for these entities
		- c. Put those categories into the metadata of the front matter of each notes in the vault. 

- 22/8/2023:
	- A new paradigm: The components that exact related to linking/categorization should not be easily touched by the editor. Otherwise, the responsibility of creating the effective schema/querying logic would fall into the user. 
	- Some strong recommendation methods should be able to be provided by the vault, so that the user has some kind of influence on the schema/categorization/querying logic. 
	- The basic categorization should be determined before anything. 

- 14/7/2023:
	- The characteristic of a good knowledge base: 
		- 1. Defined structure
			- The schema of the knowledge base can be defined by the user
		- 2. Allow changing
			- Schema can be regularly changed, and such changes could be easily applied to the vault, without yielding unexpected behavior. 
		- 3. Query
			- The data model should be adding sufficient amount of labels/attributes/links such that there are the possibility that the user could query knowledge from the vault
		- 4. Allow [[emergence]]
			- If the system become less useful when the data is increasing, it is not an effective tool for knowledge management. 
		- 5. Single mentions, multiple usage. 
			- Single fact should not be contained in multiple location. The systems should be designed in a way that multiple mentions of a single fact is avoided. 
		- 6. Machine learning
			- There are a lot of potential that machine learning could help maintaining a personal knowledge base:
				- 1. Integrity of the content: Making use of local LLM or chatGPT API, with a series of prompts

- 13/7/2023:
	- The importance of understanding "[[emergence]]" in artificial intelligence and machine learning

- 9/7/2023:
	- Problem: Discovered one more problem of "long + short" note  + entity-based" note taking
		- Say I try to include what I have learnt from one "long note" to the "short note". 
		- Say the short notes has already included 6 aspects about itself, and we embed this short notes into the long notes. 
			- Then it generates a problem: 
				- Which part of the information is mentioned / delivered from this long-notes to the short-notes?
				- This is a big problem when we aggregate many long notes. If we try to populate back the content from the short note to the long notes, the information would be overflown. 
		- Solution:
			- 1. In the short notes:
				- every points must mention the long notes. That makes the filtering is possible now at the time of populating the information back to long notes, which gives a way to solve the overflown problem. 
			- 2. In the long notes:
				- Introduce prefix "Part of `[....]`" which indicates that long notes are not fully describe most property of that entity in the notes. Or even more accurate, "X % of `[...]`" 

- Open question of the era of machine learning: 
	- Human designers always look for a low dimensional representation of the real matter. That tool could help human designer to navigate the real matter more effectively.
		- When programming, we have UML diagrams and CRC cards. They are not the programming itself but they have a big impact on the workflow of human programming designers. 
		- The time comes to the era of machine learning, we also look for the tools that can help human users of knowledge graphs to navigate themselves through the high dimensional tools that computer is using. 

- 28/6/2023:
	- The problem of the 2022 version of personal knowledge base:
		- 1. Managing the connections between nodes becomes a tough tasks since the number of node grows.
			- The value of the knowledge base lies on the connection between nodes, because connections make the users to be able to not writing about the same thing repeatedly. 
		- 2. The graph views start being lossy, or, providing less information as it size grows.
			- Graph facilitates reasoning, missing link detection. 
		- 3. While the structure can allow user to not record the same thing in multiple location, the structure do not have a rigid scheme that restrict how the user create new entities. 
			- If we do not have a scheme, then that means the knowledge base cannot be queried. 
	- Brainstorming the "algebra" within the knowledge base
		- 1. Compare concepts
		- 2. Set building 


- 26/6/2023:
	- Mason stated 3 problems in notes database:
		- 1. Lack of node diversity
			- Mathematics 
				- The underlying principle of measuring diversity is to use Shannon entropy, which is commonly used to measure the uncertainty or randomness in a dataset. 
				- Shannon entropy $H = - \sum (p_i \times log_2 (p_i))$, where $p_i$ is the probability of encountering a node of type $i$ in the database. 
			- Why $H$ is small is a problem?
				- 1. Limited data representation:
					- If certain node types are scarce or absent in the database, it becomes challenging to accurately represent or capture the full range of entities and relationships in the real-world the database is meant to model.
				- 2. Biased or skewed results:
					- When performing queries, the lack of node diversity can introduce bias or skewness in the results. 
				- 3. Inefficient performance:
					- Database often optimize their internal operations based on the structure and characteristics of the data they store. When the database lacks node diversity, it may not be able to take advantages of these optimization effectively.
		- 2. Lack of node hierarchy
			- Mathematics:
				- In graph theory, a graph consists of nodes and edges. A node hierarchy in a graph refers to a well-defined hierarchical structure, such as a tree, where nodes are arranged in a parent-child relationship. Each node has one parent. 
			- Why lack hierarchy leads to problem:
				- 1. Ambiguity:
					- It becomes difficult to determine the relationship between different entities or nodes in the databases.
				- 2. Inefficient queries:
					- It can result in inefficient queries because it may requires traversing through multiple nodes to reach the desired data. The traversal can be time-consuming and resource-intensive.
				- 3. Data integrity:
					- Node hierarchy helps enforce constraints, such as referential integrity, where relationships between nodes are defined and maintained.
				- 4. Scalability
					- As the size of the database grows, a lack of node hierarchy can make it difficult to scale and maintain the system efficiently.
			- How to fix?
				- Using primary and foreign keys
				- Define relationships between tables
				- Employing appropriate data model, like entity-relationship diagrams, relational schema.
		- 3. Lack of cross tagging
			- Cross tagging reduces the information entropy:
				- Demonstration:
					- Assume we have a database with 100 items, and each item is initially assigned a single tag from a set of 10 possible tags (Tag 1, Tag 2, ..., Tag 10). 
						- $H_{init} = - \sum_{i=1}^{10} {\frac{1}{10} log_2 (\frac{1}{10})}$
							- $log_2(0.1) = -3.32$
							- So $H_{init} = - 10 \times  0.1 \times -3.32 = 3.32$ (bits) 
					- Now, consider the process of adding a second cross tag to each item randomly, allowing for duplication or overlap with the initial tags. 
					- Because the additional tag provides more info and reduce ambiguity. Say each item has a 50% change of having a cross tag matching its initial tag.
						- Then prob of having same tag as the initial tag = 0.05
						- Prob of having different tag as the initial tag = 0.05
					- $H_new = - \sum_{i=1}^{20} {\frac{1}{20} log_2 (\frac{1}{20})}$

- Another thing to consider:
	- How to populate new information into the vault, by having such knowledge base. 
		- One possible way: 
			- Step 1: Put the information in the "type: document" note first, and mention each concept separately. That big note could provide context to each concept.
			- Step 2: When a concept is mentioned by a document-typed node, it would automatically generate a metadata "unlinked" that reminds the author to link up the context of that document.
			- Step 3: If the concept is involved more than just merely the first layer, we might have a mechanism to further propagate the "unlinked" status.


- 25/6/2023:
	- The proposing new structure of metadata of markdown file would be as follow:
		- Basic features: 
			- Title: skip
			- Date: skip
			- Tags
			- Author: It will be useful for the future (app development)
			- Content type: tutorial / research paper / snippet / case study
			- Tools / technology
			- Source: For some entities, it is useful
		- Machine learning
			- Topic:
				- Leave it to BERTopic
			- Alias / Synonyms:
				- Named 
			- Missing link prediction:
				- Graph embeddings?
			- Summary: 
				- It will be a task for AI
			- Level of expertise: 
				- Leave it to 
			- Categorization:
				- Leave it to AI
			- Hierarchical organization:
				- Leave it to AI
			- Inference mechanism
		- Object relations 
			- Inheritance (Parent/child relationships):
				- Capturing inheritance relationship
					- eg: `Tesla car` is inherited from `car`
			- Composition: 
				- Capturing composition relationship
					- eg: neural networks are composed by layers of neurons
			- Association (Related concept):
				- Capturing association relationship
				- That should be an automatically filled column.
				-  Key part of the metadata scheme. How to populate it?
			- Realization / Examples: 
				- Create a link to such association entity notes
				- For some entities, it is useful
		- Metrics 
			- Maturity of the node:
				- It will be a task for AI
			- Status
			- Unstructured ratio:
				- Use a script to run over each entity of the vault. Calculate how many percentage of the content is considered "unstructured".
			- Last update:
				- That is useful column for maintenance purpose
			- Page rank
	- Comparison between objects:
		- Problems:
			- Apart from dealing with the alias problem of concept entity names, we also need to deal with the alias problem of the attribute names. 
			- There should be a way to compress the properties of the unstructured text into metadata, such that two nodes can be compared. 
		- Approach 1: Enriching the articles of each pages with premade questions, generating from the same language model
			- With the same pipeline to generate content, the similarity between nodes are easier to be detected. 
			- 
	- Reference portion of the note
		- 

- 24/6/2023:
	- Two possible models for knowledge base:
		- Relational database:
			- More suitable for the records that has uniform and highly structured data, such as transactions, audit, log data. 
			- Relational database is not good at highly related data, say, query more than 5 hops of associations. 
			- RB impose limits on the depth of query you can do.
			- These records are rigorous and enduring, because each records needs to be defined and adhered to from the onset, and the architecture is not meant to rapidly change over time. 
		- Graph database:
			- Because data is stored as triples, this makes graph database incredibly flexible, extendable and agile.
			- It is brain friendly, like a mind map. The way we query feels very nature. 
			- Features:
				- 1. relations can be symmetrical, or not-symmetrical.
				- 2. the computer can answer questions by inferring the knowledge graph
				- 3. ontology gives the structure / consistency for the relations and entity format. 
	- Considering using `dataview` plugin to manage the vault:
		- Using `Relational database` framework:
			- Say I can query all items in the vault by `dataview TABLE`
			- Tables: I can obtain a table by `dataview TABLE WHERE type=<TYPE>`. Now a have a list of tables. 
				- I can further breakdown one table with `dataview TABLE WHERE type=<TYPE> AND category = <CATEGORY>`, to further extract more small tables.
			- Associations: Keys 
				- With those tables, I can probably manage the 1-to-N relationship more easily.
					- Container 
						- Say I have a container "(paper) attention is all you need" that contains a list of entities.
						- So "(paper) attention is all you need" would be in `type: papers` and the `papers` table would have a metadata that query the list of headings and hyperlink that page has. As a result, In `papers` table, the paper name is the primary key. And it stores the associations with a list of entities that is not in `type: paper`.

- While knowledge graphs are good for computer to infer knowledge, an effective knowledge graphs might need millions or billions of nodes and edges to generate a good inference. SO I still use the entity-based notes.
- And I decide to work out a Relational database idea, over the set of notes in the vault. 
	- With good metadata, it enforce a more powerful searches and links between entities. 

- Today I want to design a good schema
	- Ideas:
		- 1. The schema should leave a place for the future NLP tasks. 
			- Named entity recognition (group up similar items)
			- Subset / superset identifications
			- Topic modeling 
		- 2. The schema should be able to mimic the functionality of relational database
			- Association
				- 1-to-1 or 1-to-N: foreign key
				- M-to-N: intermediate table
			- Aggregation
				- Relational database has a limitation of showing aggregation relationship.
					- In relational database model, we use keys to show 1-to-N. 
				- All aggregations are 1-to-N, but not all 1-to-N relationship are aggregations. 
			- Composition
				- Same as aggregation, need to have a special column to deal with.
				- This action in database is called "denormalization" or "materialized relationship", which might make it easier and more efficient to query and retrieve data related to the aggregation or composition. 
			- Inheritance
				- Relational database does not directly showing this.
				- One common approach is known as - using a design pattern called , "single table inheritance (STI)" or "class table inheritance". 
					- In this pattern, all subclasses share a single table, and a discriminator column is used to differentiate between different types of objects. 
					- eg:
						- `car` table and `motorcycle` table can refer to the same table, but also having other discriminator columns on each table.
				- While this approach allows the user to store objects of different types in a single table, it does not provide the full range of capabilities that object-orientated programming languages offer for handling inheritance, such as dynamic dispatch / polymorphism.
			- Generalization / Realization
				- Approach 1: Use inheritance, use common columns + discriminator columns to show it.
				- Approach 2: In the table, include a type column to distinguish between different types of objects. 
			- Dependency
		- 3. The schema should absorb the idea of relational algebra that is taught in database system. 
			- Join
			- ....
		- 4. The quality of the knowledge base also controlled by how good to grasp a notion to be a single noun / "concept"
			- For this, I need to design a good template to impose some structure on the content
		- 5. Consider object-relational mapping that bridge the gap between object-oriented programming and relational database. 
			- With ORM, you can map `database tables` to `classes` and `database records `to `instances of those classes.` This mapping enables you to perform database operations using object-oriented paradigms rather than writing raw SQL queries. 
			- Composition:
				- `Blog` object can have a composition relationship with multiple `Post` object, where each `Post` belongs to a specific `Blog`. 
			- Inheritance:
				- ORM framework can map subclass/base class specification to  appropriate table structures in the database. 
		- 6. Consider object-oriented database / document-oriented database
			- Object-oriented databases: 
				- Data is represented as objects, which consist of attributes and methods.
				- The relationship between objects are also stored, allowing for complex data structures to be represented. 
				- example:
					- objectStore, GemStone / ObjectDB ... 
			- document-oriented database
				- a type of NoSQL (Not Only SQL) database that is designed to store, retrieve, and manage semi-structured data in the form of documents. 
				- Documents are represented as JSON, BSON, XML.
				- Each document is a self-contained unit that contains all the necessary data for a particular entity or object. The documents are organized into collections, which can be thought of as the equivalent of tables in a relational database. 
				- Unlike relational databases, document-oriented databases do not enforce a fixed schema across all documents within a collection. Each document can have its own structure and fields, allowing for greater flexibility and agility in data modeling.
				- Example:
					- MongoDB, Couchbase, Apache Cassandra, CouchDB. 
		- 7. The way of taking notes should refers to JSON and XML, those kind of data structures are also specialized in storing information.

- 14/6/2023:
	- Q: Steps of using semantic model to build my own personal knowledge base
	- A (chatGPT):
		- 1. Define your knowledge base structure: 
			- Determine the categories, topics, and relationships you want to capture in your knowledge base. For example, you might have categories like "Technology," "Health," "Recipes," and so on. Within each category, define relevant topics such as "Machine Learning," "Fitness Tips," "Italian Cuisine," etc.
		- 2. Choose a semantic model framework: 
			- There are various semantic model frameworks available that can help you structure and query your knowledge base effectively. One popular option is RDF (Resource Description Framework), which represents knowledge as a graph of interconnected entities using subject-predicate-object triples.
		- 3. Identify entities and properties: 
			- Identify the entities (things) and properties (attributes) that are relevant to your knowledge base. For example, in the "Technology" category, entities could be programming languages, frameworks, and tools, while properties could include features, versions, and usage examples.
		- 4. Capture knowledge: 
			- Begin populating your knowledge base by entering information about various entities and their properties. You can start with a few entries and expand over time. Use the chosen semantic model framework to represent the information in a structured manner. For RDF, this would involve creating triples to describe relationships between entities and properties.
		- 5. Link related information: 
			- Establish connections between entities and properties to create a network of knowledge. For example, if you have an entity for "Python" and another for "Django," you can link them using a "uses" property to denote that Django is a Python web framework.
		- 6. Query your knowledge base: 
			- Once you have a substantial amount of information in your knowledge base, you can start querying it to retrieve specific information. Use the query capabilities of your chosen semantic model framework to search for entities, filter by properties, and explore relationships. This allows you to retrieve information relevant to your current needs.
		- 7. Continuously update and expand: 
			- A knowledge base is an ongoing project, so continue adding new information and refining existing entries. As your knowledge base grows, you'll be able to extract more value from it.
		- 8. Utilize software tools: 
			- To effectively manage and interact with your knowledge base, consider using software tools designed for knowledge graph management. These tools provide functionalities for data entry, querying, visualization, and collaboration, making it easier to maintain and leverage your knowledge base effectively.

- Q: can we have some automatic methods to determine the structure of the knowledge base?
- A (chatGPT):
	- Follow these steps:
	- 1. Analyze the knowledge graphs
		- This analysis involves examining the entities, properties, and relationships present in the graph. You can utilize graph analytics techniques, such as graph clustering or community detection algorithms, to identify patterns and group related entities together.
	- 2. Discover latent topics: 
		- Apply topic modeling techniques, such as Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF), to uncover latent topics or themes within the textual descriptions or attributes associated with the entities in the knowledge graph.
		- This allows you to identify high-level categories or topics that can serve as a basis for structuring the knowledge base.
	- 3. Use unsupervised learning: 
		- Employ unsupervised machine learning algorithms, such as hierarchical clustering or graph embedding techniques, to automatically group entities based on their similarities. These algorithms can analyze the relationships and properties of entities to identify clusters or hierarchical structures within the knowledge graph.
	- 4. Evaluate and refine: 
		- Evaluate the generated structure against various metrics, such as coherence, consistency, or user feedback. Assess the quality and relevance of the automatically generated categories and relationships. Refine the structure by iteratively adjusting the algorithms, parameters, or incorporating additional features until satisfactory results are achieved.

- Q: How social media website categorize and classify content?
- A (chatGPT):
	- Topic modeling algorithms:
		- 1. Latent Dirichlet Allocation (LDA) 
		- 2. User-generated categorization
		- 3. Natural language processing (NLP)
		- 4. Machine learning algorithms
		- 5. Collaborative filtering
		- 6. Image and video analysis
		- 7. Hybrid approaches

- Q: Say we build personal knowledge base with semantic model. However, the knowledge base requires regular update. When we scale up the database, it requires effort (most likely doing it by human) to maintain the links between separate notes. 
- How should I plan the personal knowledge base , such that we can have less maintenance time cost?
- A:
	- 1. Organize information using a consistent structure: 
		- Define a clear taxonomy and hierarchical structure for your knowledge base. Categorize your notes into well-defined topics or themes. This makes it easier to locate and update information because related notes are grouped together.
	- 2. Utilize a semantic tagging system: 
		- Implement a tagging system that allows you to assign relevant keywords or tags to each note. This provides additional flexibility in organizing and searching for information. When adding new notes or updating existing ones, ensure that you assign appropriate tags to maintain consistent and meaningful connections.
	- 3. Create interlinking between notes: 
		- Establish links or references between related notes. By connecting relevant concepts, you can create a network of interconnected knowledge. When updating or creating new notes, make sure to include hyperlinks or references to related notes. This helps maintain the integrity of the knowledge base and facilitates easier navigation.
	- 4. Use a consistent naming convention: 
		- Adopt a consistent naming convention for your notes and files. This convention could include dates, titles, or keywords that reflect the content of each note. By using a standardized naming approach, it becomes easier to locate and update specific notes when needed.
	- 5. Leverage automated tools and technology: 
		- Explore software tools or applications that offer automation and assistance in managing knowledge bases. There are various note-taking applications, personal knowledge management systems, and semantic web technologies available that can help streamline the process of updating and maintaining your knowledge base.
	- 6. Regularly review and update: 
		- Set aside dedicated time for regular review and updating of your knowledge base. This ensures that information remains accurate, relevant, and up to date. By consistently investing time in maintaining your knowledge base, you can avoid the accumulation of outdated or incorrect information that would require significant efforts to rectify later.
	- 7. Foster collaboration and crowdsourcing: 
		- If feasible, consider involving others in contributing to your knowledge base. This could be through collaboration with colleagues, peers, or online communities. By allowing others to contribute, the burden of maintaining and updating the knowledge base can be shared, reducing individual maintenance time costs.

- 8/6/2023:
	- When making knowledge base should learn the memory structure of computer, to optimize the searching time-cost of our knowledge
	- Properties:
		- 1. Organization and categorization
			- Computer memory relies on hierarchical structures, such as folders and directories, to organize and store data. 
			- By adopting a logical and hierarchical organization system, designers can enhance the accessibility and organization of information for users.
		- 2. Indexing and searching
			- Knowledge base designers can employ indexing methods, such as keyword tagging or metadata, to enable faster and more accurate searching within the knowledge base. 
			- This allows users to find relevant information efficiently, similar to how computers retrieve data based on specific addresses or indices.
		- 3. Data compression and optimization
		- 4. Scalability and extensivity
		- 5. Error handling and redundancy

- 6/5/2023:
	- Roadmap of understanding "universal approximation theorem" in machine learning
		- 1. Compactness mentioned in UAT.
			- 2. Compactness is a concept in functional analysis.
				- "Functional analysis has been used to develop a variety of advanced neural network structure, by providing a mathematical framework for understanding the behavior of the convolution operation and its associated function space."
		- 3. To learn functional analysis, a person should 

- 3/5/2023:
	- Naming for the concept could be: "Knowledge element map" and "Knowledge element map recovery"
	- Why it is called knowledge element map?
		- Depicts the properties of:
			- 1. Knowledge map is a dictionary
			- 2. Knowledge map supports inference that is made by problem solvers / artificial intelligence
			- 3. Knowledge map recovers its 
	- To ChatGPT:
		- Q: How chatGPT remember facts for every facts it reads? If models sentence as a sequence of words, and we just let the algorithm to record the probability of co-occurrence, it seems the it cannot achieve a very good question answering quality. 
		- A: 
			- 1. Transformer technique - ChatGPT uses transformer, which is trained on massive amounts of text data using unsupervised learning techniques. 
			- 2. Representation - During the processing, the model learns each word or sequence of words in a high-dimensional vector space, where similar words and phrases are located close together. 
			- 3. Attention - The model uses a process called attention to identify the relevant parts of the input text that are most likely to contain the answer to the question.
			- 4. Learned knowledge - 
		- Q: Learned knowledge should exists in between the connections of entities. So why representing words as vector could capture knowledge?
		- A:
			- Capturing relationships between words and phrases in a high-dimensional space. By learning the association between words and phrases that tend to occur in similar context, that allows the model to capture underlying structure and relationships within the text, including factual information, semantic similarities and syntactic patterns.
		- Q: Demonstrate how "representing words as vector" could capturing underlying factual information.
		- A:
			- 1. Say we have a corpus of text. We can use Word2Vec or GloVe to represent each word in high-dimensional representation, typically ranging from 50 to 300 dimensions
			- 2. Using vector addition to capture the relationships between words
		- Q: Why we could determine 50 to 300 dimension is a good number to represent the truth?
		- A: It is a tradeoff between the amount of information captured by the vectors and the computational complexity required to process them. 
		- Q: Approach 1 - creating a list of entities as a dictionary, and create a description for each entity.
			- Instead of learning it all with more parameters, we can transform the input training data to make it easier to learn. We can design a processor that capture the knowledge of an article into a list of entities that is mentioned in the article.  and create description for each entity encountered, and then during machine learning, the algorithm could using that list of entity as a dictionary to look up the meaning of that concept. 
		- A:
			- Advantages:
				- Reduce the dimensionality of the input data
				- Ensure the model focuses on the most relevant information in the text, by highlighting specific entities and concepts that are important for the task at hand
			- Problems (of Rule-based system and feature engineering)
				- Requires a significant amount of manual effort to design and implement the entity extraction and description generation process. 
				- This approach can be brittle and may not generalize well to new or unseen entities. If the training data does not contain a comprehensive set of entities and descriptions, the model may struggle to generalize to new data that contain novel concepts.
				- using vector representation model would be able to learn from the vast amount of human-generated training data? that is difficult to capture through manual feature engineering or rule-based system.
		- Q: Someone feeds around 100,000 Questions of human user of chatGPT and  and the corresponding answers provided by ChatGPT as training data to train a new chatGPT-like question answering engine. While the training cost and parameters of the model dramatically decreased.  
		- A:
			- Why it works:
				- 1. 
		- Q:
			- I am building up a new framework for artificial intelligence, machine learning and human learning. If we have a working routine that is going to creates a map or dictionary that group up the concepts involved in a book into a edge eccentric graph would be the optimal way for improving the reading and writing of the agent (machine or human).
		- A:
			- 

- 2/5/2023:
	- Now we already have the set of technology that could organize human knowledge in a much effective way. While educational exchanges ("knowledge communication activity, such as reading and writing books, giving and attending a lecture") 

- 1/5/2023:
	- Can we think of a "generalized" structure that could fits different types of entity into the set?
	- example statement:
		- "Set builder notation" is other kind of "formal language" in mathematical analysis, apart from symbolic representation
			- The first clause and second clause mentions the supersets and other members of that superset. 
			- Notice this statement also applies a "where-clause" / "subset" operator over "formal language" with "in mathematical analysis", to reduce the domain of that entity to make the statement more accurately represent the relevant set dependency tree.
- Updated workflow of learning?
	- 1. Study the chapter
	- 2. Extract the knowledge pieces, and write down each of them in entity-based manner.
	- 3. When writing, tries to only include directly related things in its detail. Put the indirectly related things in a separate entity. 
		- Such as "Well ordering property" is a member of "A property of subsets of the set", which is a new class of entity when we learn mathematics analysis. 
	- 4. Write the notes as complete as possible, don't hesitate writing even there are some misconceptions involved.
	- 5. Put the whole block of writing of that entity to chatGPT to check for the correctness of the description. 

- 30/4/2023:
	- Writing notes for mathematics is the hardest challenge for the edge eccentric eco-system, because mathematics itself is highly abstractive, which might makes the languages becomes highly non-overlap.
	- Even writing notes for real analysis, it is still beneficial to take notes in entity-based as a unit?

- 27/4/2023:
	- maybe create a website that allow people embed their own interpretation of the popular access files or video lectures. And people could install a add-on that could show that additional resource if that viewer is installed the browser extension.
	- similar thing:
		- catalyzeX

- 26/4/2023:
	- Actually an academic paper that is full of tables, can be represented as a pipeline graph, 
		- which tables can be represented as a matrix? 
		- and some of its node could be represented by some matrices as well?
	- eg:
		- Paper: "Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models"
		- Source: https://deliverypdf.ssrn.com/delivery.php?ID=975094099100000027064070071064121028118048034028083053113066077025103003067107089087000019119031026058055085018119109010020087027027021084004065068115008026100123074017036018117013086118092114006069068000095085069087007069080119093005108084002027095003&EXT=pdf&INDEX=TRUE

- 17/4/2023:
	- Language skipping behavior of human
	- Explains why human cannot write as detail as chatGPT response?
	- for example,
		- chatGPT could give me complete form of answer -- "Stacking: Stacking, also known as meta-learning or model stacking, is an ensemble method where the predictions of multiple base models are combined as input features to train a higher-level meta-model. "
		- Which contains "Its alias as meta-learning, or model stacking", which humans might skip documenting those alias while their mind is focusing on its usefulness. 
		- Which also contains "It is an ensemble method", which humans might skip writing it. Forcing themselves to think of the subset of the entity seems rare to human writing. 
		- While when I write the explanation by myself, 
	- Reasons human can't write this detailed note:
		- 1. Limited knowledge - They may be not aware of all the details or nuances of a particular topic
		- 2. Personal Bias - Humans may have their own  biases and perspectives that can influence their  writing. They may unintentionally omit certain details, which do not align with their own views or opinions
		- 3. Cognitive Load: Writing requires cognitive effort, which including organizing thoughts, choosing appropriate words, and structuring sentences.  Humans sometimes omit certain details due to cognitive load or limitations in working memory. 
		- 4. Writing styles: Some person may prefer concise and straightforward writing, while others may be more detailed and elaborate. 
		- 5. Time constraints: Time budget can result in omission of certain details. 

- Examine how well RDF could capture knowledge
	- Original text: 
		- "Label: In machine learning, a label typically refers to the output or target variable that a model is trained to predict. It is the desired outcome or class that the model aims to assign to new, unseen data points. For example, in a binary classification task where the goal is to classify emails as spam or not spam, the "spam" or "not spam" label would be the target variable that the model tries to predict based on the input features."
	- RDF representation:
		1.  Subject: "Label" Predicate: "Refers to" Object: "Output or target variable in machine learning"
		2.  Subject: "Label" Predicate: "Typically refers to" Object: "The output or target variable that a model is trained to predict"
		3.  Subject: "Label" Predicate: "Represents" Object: "The desired outcome or class that the model aims to assign to new, unseen data points"
		4.  Subject: "Label" Predicate: "Is assigned by model to" Object: "New, unseen data points"
		5.  Subject: "Label" Predicate: "Used in" Object: "Machine learning"
		6.  Subject: "Label" Predicate: "Example of" Object: "A binary classification task"
		7.  Subject: "Binary classification task" Predicate: "Has a goal to" Object: "Classify emails as spam or not spam"
		8.  Subject: "Binary classification task" Predicate: "Has labels" Object: "Spam or not spam"
		9.  Subject: "Label" Predicate: "Is predicted by model based on" Object: "Input features"
		10.  Subject: "Input features" Predicate: "Are used for" Object: "Predicting the target variable by the model"
	- Comment: "I see there are some problem of RDF, in your given answer"
		- 1. Objects and predicates can be grouped into some subset groups to improve consistency
			- In your RDF triple 1 and RDF triple 2, we see there are duplicated in meaning. 
			- In your RDF triple 1 and 2, we see there are inconsistent in the "predicate" and "object" naming
		- 2. Some objects are wrapping up another compound structure as an object
			- In the object of your RDF triple 3, the wrapped structure is "model" that "assign new, unseen data points with labels". And there is a pointer for Subject "label" to points to the component "data points with labels" of the whole subject "model". 
			- In this context, "structure" means a thing that is composed on more than 1 components to form up itself. 
			- The problem of wrapping the whole structure into an object is, that could creates a numerous combination ($C_r^n$) of new objects, from the same structure, depends on how complicated ($n$) the structure and how many components $r$ are mentioned. 
			- But if we propose there are a composition relationship between "data points" and "labels", in the context of "machine learning model",, with this data model, we don't need to wrap up structure, which won't creates numerous varies form of "objects" in RDF, which adds a lot of inconsistent of the naming of "object"
		- 3. RDF itself is not a good structure to communicate there is data underlying relationship structure
			- In 'label' - 'use in' - 'machine learning'. machine learning is just the context of where 'label' appear most. And that is a bad communication format if we just link up an object to a context, as same as how we link up object A to object B. 
			- We need other kind of grammar to represent the underlying relationship knowledge. 

- specialized RDF vocabularies
	- more specialized RDF vocabularies or ontologies can be used to provide more specific representations of relationships, depending on the domain or application. These specialized vocabularies may define additional RDF properties, classes, and relationships that align with the semantics and structure of the specific domain being represented. 
	- 

- [[Ontology]]

- Example:
	- Schema.org
	- DEpedia

- Figure: Blueprint of schema.org
![[Pasted image 20230417020537.png|400]]

- How machine learning could reduce redundant node / edges?
	- 1. Alignment
		- finding mappings between entities and relationships in the knowledge graph and the corresponding entities and relationships in the ontology. Alignment algorithms typically use similarity measures or embedding techniques to establish correspondences between entities and relationships in different knowledge representations
	- 2. Vocabulary reduction:
		- eg: "USA," "United States," and "US". 
			- The machine learning algorithm can learn to map these terms to a single preferred term, such as "United States of America," based on the ontology's vocabulary.
	- 3. Entity resolution:
		- Also known as record linkage, deduplication
		- Identify and merge duplicate or similar entities in the knowledge graph and the ontology.
		- This involves comparing the attributes or features of different entities and determining their similarity or dissimilarity.
		- For example, if the knowledge graph contains multiple entities representing the same person with slightly different spellings or variations of their name, the machine learning algorithm can learn to identify and merge these entities into a single entity based on the ontology's entity resolution rules.
	- 4. Relationship inference:
		- Machine learning algorithms can learn from this domain knowledge and make inferences to fill in missing relationships in the knowledge graph, by leveraging the semantics of the ontology. 
		- For example, if the ontology defines that "is-a" relationship represents the hierarchical taxonomy of entities, the machine learning algorithm can use this information to automatically infer "is-a" relationships in the knowledge graph based on the existing entities and their attributes.

- Taxonomists versus ontologists
- Similarity
	- 
- Difference
	- Taxonomists - 
		- Focus
			- focus on organizing and classifying objects or entities based on their shared characteristics. 
			- Often create hierarchical structures called taxonomies.
		- Methodology
			-  use empirical observations, measurements, and morphological characteristics of objects to create taxonomies. Taxonomies are typically based on a bottom-up approach, where objects are grouped and classified based on their physical characteristics or shared features.
		- Scope:
			- typically work within specific domains or disciplines, such as biology, botany, or zoology, and their taxonomies are often domain-specific 
		- Representation:
			- Taxonomies are typically hierarchical structures that organize objects or entities into nested categories or classes based on their shared characteristics. Taxonomies often have a tree-like structure with parent-child relationships between categories, and they may not capture more complex relationships or meanings between concepts.
	- Ontologists - 
		- Focus
			- Organizing and classifying knowledge based on their conceptual relationships and meaning. 
			- Work in the field of knowledge representation and formal semantics
			- Capture relationships between concepts, entities and relationships in a specific domain
		- Methodology:
			- Use formal or conceptual modeling techniques to create ontologies. Ontologies are based on a top-down approach, where conceptual relationships and meanings are defined explicitly using formal languages or modeling frameworks, such as RDF (Resource Description Framework), OWL (Web Ontology Language), or other ontology modeling languages.
		- Scope:
			- can work across various domains and fields, including but not limited to science, healthcare, finance, e-commerce, and more. 
		- Representation:
			- Ontologies are typically richer and more expressive representations that capture not only hierarchical relationships but also other types of relationships, such as part-of, is-a, and has-property. 
			- Ontologies can also include axioms, rules, and logical constraints that define the semantics of the concepts and relationships in a formal and explicit manner.
			- express with set theory 


- 14/4/2023:
	- Twins manifestation of graph 
		- 1. Basic structure:
			- Entity viewport = Node-eccentric perspective of knowledge graph
			- Edge-eccentric perspective - Store knowledge and treat edges as the basic unit of knowledge
		- Discovering more complex graph structures when putting the graph into some algorithms
			- identifying synonyms, acronyms, or other equivalent terms in the ontology. This can help in normalizing the terminology used in the knowledge graph and aligning it with the ontology's preferred terms.

- Subsets of relationship
	- Spatial Relationships
	- Temporal Relationships
	- Causal Relationships
	- Part-Whole Relationships
	- Hierarchical Relationships
	- Associative Relationships
	- Functional Relationships
	- Social Relationships
	- Communication Relationships
	- Ownership Relationships
	- Taxonomic Relationships
	- Semantic Relationships
	- Behavioral Relationships
	- Behavioral Relationships
	- Collaboration Relationships
	- Reputation/Trust Relationships
	- Compliance Relationships
	- Measurement Relationships
	- Evolutionary Relationships
	- Participation Relationships
	- Economic Relationships
	- Legal Relationships
	- Influence Relationships


- 13/4/2023:
	- In graph representation of textual meaning, we must establish a way to distinguish "entity" and "attribute of an entity".
	- eg:
		- Intuition of neural network versus "intuition" as an attribute of "neural network"?
		- "machine learning paradigm", for this bigger term, "paradigm" is the main subject, or the "machine learning" is the bigger subject?

- Conversation with chatGPT
	- Q: 
		- I have an idea that turning human notes into node-edge graphs. Review my idea. While the number of entities could be infinite, the number of the types of edges should be managed, such as to be managed in hierarchical relationship. Because "kind of relationships" should be finite. With finite edge type, we can let the computer to use some algorithm to convert natural text into knowledge graphs, which the edges are more probable fall into predefined categories. Predefined categories would gives the edge meaning, which allow the computer to compute the graph more efficiently.
	- A: 
		- Your idea of converting human notes into node-edge graphs using predefined categories for edges is intriguing. By limiting the number of types of edges and organizing them hierarchically, you can create a structured representation of knowledge that can be processed more efficiently by algorithms.
		- Having finite edge types can provide meaning to the edges, allowing the computer to understand the relationships between entities in a more organized way. This can enable the computer to perform various operations on the graph, such as querying for specific information, reasoning about relationships, and making inferences.
		- By using algorithms to automatically convert natural text into knowledge graphs based on predefined categories, you can potentially automate the process of creating structured representations of information from unstructured text. This can be particularly useful for tasks such as information extraction, text summarization, and knowledge representation.
		- However, it's important to consider some potential challenges with this approach. Natural language is complex and often ambiguous, and mapping it to predefined categories of edges may not always capture the nuances and subtleties of human knowledge accurately. Additionally, defining and maintaining a comprehensive set of predefined categories can be challenging as knowledge and language evolve over time.
		- Nonetheless, with careful consideration of these challenges and appropriate design and implementation, your idea has the potential to create a valuable tool for converting human notes into structured knowledge graphs, enabling efficient processing and analysis of information.
	- Q:
		- is there anyone tried to do that
	- A:
		- Yes, there are existing approaches and tools that aim to convert human notes or unstructured text into structured knowledge graphs. These approaches often involve natural language processing (NLP) techniques and graph-based algorithms to extract information from text and represent it as nodes and edges in a graph.
		- For example, some knowledge extraction and graph construction methods use named entity recognition (NER) techniques to identify entities such as entities, events, and relationships from text, and then represent them as nodes in a graph. The relationships between these entities can be represented as edges with predefined categories, which provide meaning to the relationships.
		- There are also approaches that utilize machine learning algorithms to automatically classify and map text into predefined categories of edges in a knowledge graph. These algorithms can be trained on annotated data to learn patterns and relationships in the text, and then use this knowledge to automatically create graphs from new text.
		- Some examples of existing tools that use similar approaches include Stanford's OpenIE, GATE (General Architecture for Text Engineering), and GraphAware's Hume. These tools utilize NLP techniques, graph-based algorithms, and predefined categories of edges to convert unstructured text into knowledge graphs.
		- It's worth noting that the field of knowledge extraction and graph-based representation of information is an active area of research and development, with ongoing efforts to improve the accuracy, efficiency, and scalability of these approaches.
	- Q:
		- But hierarchal edges type is not the same as NER
	- A:
		- You are correct, hierarchical edge types are different from named entity recognition (NER). NER is a technique used to identify and classify named entities such as people, organizations, and locations in text, whereas hierarchical edge types refer to predefined categories that represent relationships between entities in a structured knowledge graph.
		- In the context of your idea, hierarchical edge types would involve organizing the relationships between entities in a graph into a hierarchical structure, where relationships are categorized based on their type or nature. This can provide additional meaning and context to the edges in the knowledge graph, allowing for more efficient processing and analysis of the graph.
		- For example, instead of having a single edge type such as "related to" between entities A and B, you could have hierarchical edge types such as "parent of," "child of," "sibling of," etc., which provide more specific information about the relationship between entities.
		- Implementing hierarchical edge types would require defining a predefined set of categories that capture the common types of relationships in the knowledge domain of interest, and then mapping the extracted relationships from text or other sources to these predefined categories. This can be done using rule-based approaches, machine learning algorithms, or a combination of both.
		- While NER and hierarchical edge types serve different purposes, they can be complementary in creating structured knowledge graphs from unstructured text. NER can be used to identify and classify entities, while hierarchical edge types can provide meaningful relationships between these entities, resulting in a more structured and semantically rich representation of knowledge in the graph.
	- Q: 
		- is there anyone tries to build "hierarchal edges type"
	- A:
		- As of my knowledge cutoff date in September 2021, the concept of "hierarchical edge types" as you described in your idea is not a commonly used approach in the field of knowledge graph construction from unstructured text. The traditional approach is to use predefined edge types or predicates to represent relationships between entities in a knowledge graph, without hierarchical categorization.
		- However, the field of knowledge representation and extraction is constantly evolving, and there may be ongoing research or experimental efforts that explore the use of hierarchical edge types or similar concepts. Research in natural language processing (NLP), semantic web, and graph-based representation of information is continuously advancing, and new approaches and techniques are being developed and explored.
		- It's possible that there may be research or projects that have experimented with or explored the idea of using hierarchical edge types to represent relationships between entities in a knowledge graph. These efforts may be published in research papers, technical reports, or presented at conferences or workshops in the field of artificial intelligence, information retrieval, or semantic web.
		- To stay updated with the latest developments in this field, it would be beneficial to review recent research literature, attend relevant conferences or workshops, and explore research repositories or databases related to NLP, knowledge representation, and graph-based representation of information. Additionally, connecting with researchers or practitioners in these areas may provide insights into any ongoing or past efforts related to the use of hierarchical edge types in knowledge graph construction from unstructured text.







- 11/4/2023:
	- Hypernym / "part-of" relationship:
		- Semantic relations between a term belonging in a set that is defined by another term and the latter
		- Hypernym means word A is more general than word B.
			- eg: Color is hypernym of Purple
			- eg: Digestion is hypernym of Digest 
	- Hyponym


---
### The parts below are already considered in another notes 

- Type A notes:
	- As a metadata, to track the progress of linkage, the tag should be followed the longer description that is copied from the source. 
- Type B notes:
	- All descriptions should be marked it came from which "type-A" notes 


- 9/4/2023:
	- Content prediction:
		- Say I am putting the concepts from [[Course 1 of 4, Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning]] into separate entity notes. 
		- I might write the content for each entity individually. But that would lose the context meaning of the course designer. The concepts in the same section should be closely correlated to each others. 
		- Computationally, Say
			- C: context node
			- E: entity node
			- Then when writing E, we shall infer the meaning of E by inferring E-->C-->E, and then mention each other E that is close to that E .

	- The source that optimized for conveying the meaning for a dictionary of entities
		- Courses, notes, books usually performing well in delivering the concepts to the reader. It is because the context of the books allowing the author skipping some details to make the flow concise. 

	- The source that optimized for computing / solving problems
		- When solving problems, the indicator of fast processing is, how many related stuffs are recalled, AND how many not related stuffs are not recalled. 
		- Searching "courses, notes, books" are not the way to help us to solve problem effectively. 
		- Entity-based notes contains:
			- 1. Its connection to other entities (its functionality manifest by interaction)
			- 2. (Read 5 regions of knowledge)
			- 3. 

	- Edge-centered learning 
		- While entity-based writing could retain the information more effectively than plain notes writing. Most of the knowledge actually exists in between interactions between entities. 
		- In machine learning, the connections between neurons in a neural network are crucial for the network's ability to learn and perform tasks. 
		- During training, the weight of those connections between entities are adjusted 

- 10/4/2023:
	- The problem in encoding
		- The problem of linkage begins every time we write things cooperate with existing vault.
			- Say, today I am updating the content of [[Course 1 of 4, Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning]]
			- I write each word according to what I have in hand. But the vault don't know how to interact with it.


- 8/4/2023:
	- While we have 2 kinds of notes now
		- Kind A:
			- detailed course notes, which provides context; and 
		- Kind B:
			- abstract entity-based notes, which speeding up the reasoning. 
	- There is lack of a tracking log that helps the users to know the status of the content in contextual notes is linked to certain abstract notes. 
	- Say, each section in the kind A notes we also would have a status section which notify the user whether that section is connected to some abstract notes.

	- Extending this concepts
		- We can also do this checking to a level of "each sentence".
		- Say, each sentence should whether have:
			- Kind A -  belongings of entity 
				- (i.e. that sentence would be an attribute / description of some entities)
			- Kind B -  Itself is the topic sentence of discussing one entity
				- (i.e. that sentence triggers us to create an entity, or link that block of content to existing entity content.)


- 6/4/2023:
	- Vicuna
		- Vicuna managed to apply shareGPT's conversation data to train another LLMs. 
		- In the case of Vicuna, we see that most learning effort is on the process of making good quality source for training. 
		- For chatGPT, the training data is not structured and well answered. So the required hardware and required data also grows exponentially. 
- parameters while learning 
	- As a human learner, deliberately recognize the number of possible options or concepts to be learned, can be beneficial for human learners.
		- It is like change the hyperparameter
		- having an understanding of the number of possible options or concepts to be learned can be beneficial for human learners as well.
		- By understanding the scope and complexity of a learning task, learners can adjust their learning strategies and allocate their time and resources more efficiently.
		- Having an awareness of the number of possible options or concepts can also help learners identify and prioritize the most important information to learn, rather than getting bogged down in details that may not be as relevant or useful.
	- Strategies for unsupervised learners:
		- 1. Use prior knowledge or context to help them interpret new information
		- 2. Seek out additional resources or perspectives to gain a broader understanding
		- 3. Engage in reflection and self-assessment to monitor and improve their learning progress
	- Transformer, not learner
		- Transformers (takes data, generates refined data)
		- Learners (takes data, generates predictions)
		- Transformers could cooperates, while learners cannot cooperates. 
	- Types of transformers:
		- Feature extractors
		- Dimensionality reducers
			- PCA
			- SVD
		- Data cleaners
	- Types of learners (either supervised or unsupervised):
		- Regression models
		- Classification models 
		- Neural networks

	- Focus on difficult examples -- is that valuable?
	- Valuable (not so):
		- Mostly no, because whole decision progress is an average. If we put too much weights on difficult examples, it makes the rest of decision making neurons for easy examples also moving  around. 
	- Not valuable:
		- This is because the difficult examples may not be representative of the general distribution of data, and the model may overfit to these examples and perform poorly on new, unseen data.
		- some difficult examples may be outliers or noise in the data, and incorporating them into the training process may harm the model's performance rather than improve it
	- Active learning
		- Active learning is a paradigm in which a learning algorithm iteratively queries an oracle (i.e., an expert or human) to obtain the labels of a carefully selected set of examples. The algorithm uses the labeled examples to improve its performance in subsequent iterations.

- How to create neural networks prone to noise and outliners
	- Neural network structures with a large number of parameters
		- They are more prone to overfitting and may be more sensitive to noise and outliers in the data. This is because they have the capacity to memorize the training data, including the noise and outliers, instead of learning the underlying patterns in the data.
	- Deep neural networks:
		- the gradient signal can become very weak or vanish in the deeper layers, making it difficult to update the parameters correctly.

	- How to create robust networks deal with noise and outliers
		- Convolutional Neural Networks (CNNs)
			- Parameter sharing
				- the same set of parameters is used to process different parts of the input, which enables the network to learn local features that are robust to variations in the input
			- Pooling layers
				- Down-sample the feature maps produced by the convolutional layers. This reduces the sensitivity of the network to small variations in the input 
			- Regularization
				-  CNNs often use regularization techniques such as dropout or weight decay to prevent overfitting and improve the generalization capability of the network. 
			- Transfer learning
				- CNNs can be pretrained on large datasets such as ImageNet, which enables them to learn a set of general features that can be applied to a wide range of tasks. 
		- Recurrent Neural Networks (RNNs)
			- Varying length:
				- . Unlike feedforward neural networks that take fixed-size inputs and produce fixed-size outputs, RNNs can take inputs of varying lengths and produce outputs of varying lengths. This makes RNNs well-suited for handling noisy or outlier data that may be present in the sequence.
			- Recurrent architecture
				- the output of a previous time step is fed back as input to the current time step. This allows the network to maintain an internal state or memory of previous inputs, which can help it to handle noise and outliers in the data.
			- Long-term dependencies
				- RNNs can learn long-term dependencies in the sequence, which can help to improve the generalization capability of the network.
		- Adversarial training
		- Residual network


- 5/4/2023:
	- Data formats, gene expressions, grammar of languages. And the value of constraints to the communication system. (See more description on "vault design" page)


- 29/3/2023:
	- Ways to improve database performance
		- Query optimization techniques
			- Finding the most efficient way to execute a given database query
			- The goal is to minimize the time it takes to execute a query, while maximizing the utilization of system resources such as CPI, memory and disk I/O.
			- General methods:
				- 1. Indexing: 
					- Quickly locate the rows that match a given query. 
				- 2. Partitioning: 
					- Dividing a large table into smaller, more manageable pieces called partitions. 
				- 3. Caching: 
					- Storing frequently accessed data in memory so that it can be quickly retrieved. 
					- Access memory is in nano-second, and access disk is in the speed on mini-second.
				- 4. Query rewriting:
					- Transforming a complex query into a simpler, more efficient query that produces the same results. By eliminating unnecessary joins and subqueries, or reordering the execution of certain parts of the query. 
				- 5. Parallel execution:
					- Breaking a query into smaller parts and executing those parts simultaneously.
				- 6. Materialized views:
					- Precomputed tables that stores the results of a query. That help to reduce query execution time by eliminating the need to execute the query every time data is requested. 
				- 7. Statistics
					- Database systems use statistics to estimate the cost of executing a query. 
				- 8. Join optimization
					- Use the optimal join algorithm and join order
					- Minimize the amount of data that needs to be processed and improve performance. 
		- Indexing methods (locating)
			- Creates a data structure that helps in faster data retrieval.
			- Indexing = Creating an index on one or more columns of a database table. 
			- An index stores a copy of a portion of the data from a database table, in a separate data structure, to make it easier and faster to retrieve data from the table.
			- 1. Faster data retrieval:
				- It allows database systems to retrieve data from a table more quickly.
			- 2. Reduced disk I/O:
				- Reduce the amount of disk I/O that is required to access data in a table. 
				- Index provides a more direct path to the data that is being requested.
			- 3. Improved query performance:
				- When queries are executed against a table that has an index, the database engine can use the index to quickly locate the data that is required by the query. 
			- 4. Faster sorting
				- Indexing can improve the performance of sorting operations. Sorted index can be worked out more efficiently.
			- 5. Improved concurrency
				- Reducing the amount of time a table is locked when data is being accessed or modified. 
		- Data compression algorithms
			- 1. Reduced disk space usage
				- Additional storage space could slow down database operations
			- 2. Faster data access
				- Compressed data can be read and write to disk more quickly than uncompressed data, as it takes up less space and requires fewer I/O operations. 
				- So data retrieval, updates and backups would be faster.
			- 3. Improved memory usage
				- As it requires less memory to store and manipulates. 
			- 4. Lower network bandwidth usage
				- Reduce the amount of data that needs to be transferred over a network.
	- Ways to improve performance of machine learning:
		- 1. Feature engineering
			- Selecting and transforming the input variables to create a more informative representation of the data for the machine learning algorithm.
		- 2. Model selection and tuning
			- Choosing right model for a particular problem and tuning its hyperparameters can significantly improves performance.
		- 3. Ensemble method
			- Combining multiple machine learning models to improve predictive accuracy
		- 4. Data augmentation
			- Creating new training examples by manipulating or generating additional data. 
		- 5. Transfer learning
			- Reusing knowledge gained from one machine learning task to improve performance on a different task.
		- 6. Hardware acceleration
			- Using specialized hardware speed up training and inference time.
		- 7. Distributed computing
			- Distributing inference workload across multiple machines. 
	- Venn diagram for the above examples:
		- 

- 27/3/2023:
	- 1. What I need to do is -- how to change the pattern of humans to store their information, which is similar to how neural networks store information. 
	- 2. The question becomes "missing link detection" for human user when the user go along with the tool. Say we have a tool that maps the relevant concepts as a map. 

- 2/3/2023:
	- [[2 March 2023 - ways to organize information such that a NLP model could retrieve information effectively and consume less memory]]



- 2/3/2023 (2):
	- [[2 March 2023 Query and query-related data structure]]

- 11/2/2023:
	- language models like chatGPT are based on transformer models. So, there are still a range of  design space that producing a good representation of text corpus, before giving the model to process the data.  
		- So we need to learn about what is transformer model.
	- "Pattern extraction + pattern dictionarying + pattern application" as a way of abstract thinking
		- In human language studies, we have a set of entities that is describing how people enhance its own expression by some formulas. Overall, learning the discovered patterns is a more effective way of learning knowledge. 
		- In learning human language, we have:
			- 1. Learn the annotation / labels for the smallest components
				- a. Grammatical annotation of single word: Noun
				- b. Small categories of words: Animals, Temperature etc.
			- 2. Discover some patterns from the corpus, directly. 
				- Say you want to speak like How famous person, like, Obama. You collect a lot of corpus from him.
				- You find the pattern within the corpus. Pattern searching only possible if the list of "basic pattern" is already inside the agent.  (This step is important)
				- For human, it is easy to discover new patterns from old patterns they have already got.  (This step is important)
- My point is, learning the midway pattern is the main objective for human.
	- Which contributes to:
		- Sense of humor of a person
		- Artfulness of a person
- Person who are having higher intelligence in art, humor, math sense, are those who are not only could sum up the pattern from repeated event, but also coming up with hypothesis and to verify it. 
	- A humor person would tries to move attention to an angle that is unreasonable, then tries to verify it very soon.
	- A scientist would tries to make hypothesis that's something not merely exists. 

- Suggestion of research direction:
	- Collect a list of standup comedy from YouTube. Extract the soundtrack (for laughter) + its subtitle, and then analyze the pattern of creating jokes. And then try to verify the joke pattern with GAN. 



- 6/2/2023:
	- Learning/successful-training means "You covered up a part of the information of the material, and your system could still predict what it is covered up correctly.".
	- We start off by providing information to the agent. 
	- Then we cover up some information out of the whole content. Then stimulate the neural network to see the pattern.   
	- The result of learning would be: the agent have ways to use the remaining linkage of information of him to produce the correct prediction of missed information. 

- 2-2-2023:
	- Source: https://www.youtube.com/watch?v=dtNzSdwqzCc
	- 1. Examples and counterexamples 
	- 2. Visualization
		- In mathematical understanding, visualization is important to depict the concept?
	- 3. Why (purpose of it) then how (the details of the math)
		- Asking why - poking out more context about the problem.
		- Asking how - most knowledge exists in the edges between entities, not in the entities itself. 
	- 4. Learning-by-teaching
		- What is teaching
			- 1. Planning what material to (versus not to) present
			- 2. How to frame and express the material
			- 3. Attempting to provide organizational structure
		- Retrieval practice
			- First remember and retrieve the to-be taught material. 
			- Secondly, plan what material to present, thinking about how to frame and express the material. Attempt to provide organizational structure. 
	- 5. Practice writing a complete explanation around a concept
	- 6. 

----
### The following parts are already applied on "vault design" page

 - 22-12-2022:  ^a9f269
	- Every events in life, every notes, projects etc are repeated process that's like cutting cookies with tools. There will be benefits to create "structural typing" for each of them. 
	- For notes:
		- 1. Each notes should follows an architype. And all notes should belongs to a collection. 
		- 2. The collection would facilitate discovery of the notes, which could provide a higher speed retrieval than relying on the search bar. 
	- For materials inside the note:
		- 1. Some repeated parts should be also taken out and design some architypes for them. Such as coding example templates. 

- 17-10-2022:
	- Currently the way of updating the vault:
		- 0.  We have a meta pages that links to all notes that are in the same type. eg: "functions index" for all notes that are attributed as "function". 
		- The graph accumulates knowledge batch by batch. 
		- A new batch:
			- 1. Generate new notes with no limits, and all of them located to a temporary location "New Notes"
			- 2. Classify how each notes should be located, drag them to the folder to gives them a type.
			- 3. Go to the meta page for each type, to link each newcomer notes that haven't linked to the meta page. 

- 2-10-2022
	- Steps of using this value, my experience
		- Document zone 
			- 1. Gather a lot of papers from internet, and then create a note page as a container for each of them
			- 2. Write notes in each containers 
			- 3. Find new concepts and then create a atomic notes for each concept, and then in the atomical concept notes, refer back its property to each papers notes.
		- Atomic note zone
			- 1. Gather ideas from documents

- Dimensionality of information
	- Library ( = a set of books, papers")
	- Paper  ( = a set of reasonings. with a categorization of "Subject / theme")
	- Reasonings ( = a set of relations and entities pairs / proposition + implications, with a categorization of "Arguments / Lemma")
	- Relations and entities ( = a set of nodes, with a categorization of "node type" + a set of links, with a categorization of "link type")
		- Metadata (eg: author, date) co-exists with the smallest entities-relation pairs. 

- Discussion of the above structure
	- We take Library > paper as an example. 
	- 1. Each level,  each objects (eg libraries, books) would have a specific purpose (eg: geotechnical library: = a collection of books
	- 2. but each objects would have its own discreteness, and they differentiate themselves with a categorization header,  and we can use these headers to sort out some order for the set of object. 
	- 3. The upper level domain provides a "context" for each lower level domain.

- Requesting a common functor that extract information in each level:
	- Suppose we are doing a "sense making" task, which we search for useful information from the library at first, and then publish and reconstruct a new paper in the seond. 
	- The dimensionality of the set of knowledge space is: 
		- Library > Papers and books > Arguments > 

- 1-10-2022:
	- It is possible to follow [[category theory|category theory]] to organize the folder ordering of notes. 
		- 1. Since programming is a kind of thing that [[category theory|category theory]] capable to analyze, we have:
			- Some obvious functional notes like [[Decoder]], [[GloVe]]
		- 2. Observed there are more than 1 type of category in the notes. 
			- Some are not included in the category of "machine learning programming". EG: GloVe could be an object in machine learning programming script, but "Loss of information" would not be an object in a machine learning programmign script.

- 30-9-2022: modified

![[Pasted image 20220930030853.png]]
- Figure: the structure of the note right now. 

![[Pasted image 20220930030524.png]]
- Figure: The workflow of the obidian right not. 

"依家用緊Graph based note taking, 

(由左到右):

例如你想學一個topic, 

開一個sprint page先, download 5份 material / paper, 每份skim 10%, 掉走最廢果份, 再skim 10%, 再掉, 如是者揀到最好果兩份material / paper, 睇哂佢

然後響obsidian開兩type notes, 一type係knowledge script, embed番個pdf, preserve 哂成份paper 既內容, 寫番自己既notes落果版度

然後另一type notes係entity based 既notes, 只mark關果個entity事既野, 而且所有內容都係refer to Type 1 notes

最後最右邊個local graph就係result,  (可以較depth去增加減少neighbor數量)"

"reference 番來源好重要, 原因係筆記入邊, 大部分句子都唔係自己寫, 要make sense就要睇番原文

然後name entity based graph 係暫時我覺得最好既筆記款, 每頁只提自己, 份筆記既「可重組性」係最高"



- 28-9-2022: modifed

---
- 1. Graph-centered approach
	- (A) Each vault only care about one matter. Machine learning = A specific field => Create one specific vault for it. 
	- (B) Create a number of folder, that is not going to be shown in the graph. Those folder contains a collection of "knowledge script" that tries to script the logical flow, the storyline of the author's understanding into an article. The logical flow cannot be fully represented by a list of triples, or a collection of self-specifying named entities. 
	- (C) Create a number of folder that only contains a collection of named entity that only talk about themselves.
2. Paper entities part:
	- (A) Create markdown file for each paper. Try the best to download the paper into the vault's attachment folder, link the pdf to the markdown file. and then highlight / marks the pdf. After one iteration of reading, transport the useful concepts to the named entities files.  
3. Named entities part:
	- (A) Each entity should have at least one node type. Node type is represented by "what folder they are located at".
	- (B) A good node type classification should be (1) No intersected element between classes. (2) The sum of all types should be able to capture all elements, (3) The classification is useful for navigating the dependency structure of the collection of nodes. 
4. Content for each entity files:
	- (A) It is always the best to maintain each entity markdown to be JSON like tree-like structure. The "key" of each properties are directly related to the nameof the entity file (the subject of the whole document). 


---
- 25-9-2022: created


- How to use Obsidian?
	- One vault tries to create a graph that could represent all concepts for the domain of "machine learning". Many users ends up give up to maintain the graph because of the workload of classifying the entities apart. 
	- That is difficult because in most of the time we are not that focusing on studying one specific object. 
	- While keeping the control of attention flow that the original "knowledge script" (rather it is a lecture, or an article), we creates a flexible node based interlinking data structure representing our understanding of the matter. 
- Design of the graph
	- Each node should be a named entity, and then each entity should have at least one node type. 
	- A good node type classification should be (1) No intersected element between classes. (2) The sum of all types should be able to capture all elements, (3) The classification is useful for navigating the dependency structure of the collection of nodes. 
	- Content control: 
		- the most difficult think is to sort out text from a sources to N notes. Which takes mental efforts to do that.  
		- and in each named entity there should have some ways to guide how to write each paragraph such that each paragraph could be further linked to another notes. 
---
- Proposal: 	
	- My inputs folder:
		- All courses (each folder has an index page and links all subpage + provide a rouge subject classification)
		- All papers and html
	- My node classification proposal (for distilled parts):
		- Concrete facts
		- Functional model (takes in certain facts, and generate some kind of judgments / classifications) (entailed with some qualities have improved)
		- Quality model (That provide supervision for the changes of behavior model)
		- Problem patterns (i.e. a sequence of general steps) (eg: the general step that solve some problems)
		- Implementations (the worked example that tries to following problem patterns)
	- Is that classification good?
		- For inputs: one paper could have multiple authors, and one paper could also covers multiple topics. --> The topic classification is not concerning on input part. The author part, I should tag the author in the beginning of each input page. Person by person tag. 

---

- The suggested step of populating the vault:
	- When want to learn something
		- 1. Have some concerns (That open at least one thread for "implementation" folder)
		- 2. Read some "knowledge scripts" (That produce at least one inputs for "All inputs")
		- 3. Read the script about how to solve it (That produce "Problem pattern")
		- 4. Sort out "facts", "functional models", "quality and attribute models" that are mentioned in the paper. 
	- When want to produce some knowledge script:
		- 1. Imports written concept in the vault, try to reduce the amount of impromptu writting, which increases the productivity of the writing. 