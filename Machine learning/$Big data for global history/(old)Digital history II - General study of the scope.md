

---

---
## Library resource - What is digital humanities?
- Source: https://libguides.lib.cuhk.edu.hk/Intro-DH/home
- Data collection
	- Digitalization
	- Text encoding
	- Data extraction
- Data interpretation and analysis
	- Text mining and analysis
	- Spatial and temporal analysis with GIS
	- Image analysis
- Data visualization

- Online display
	- Omeka
	- Story maps
- Multi-media production tools
	- Blender
- Text analysis
	- NVivo
	- Voyant
	- Markus
	- Corpro
- Selected DH projects
	- https://libguides.lib.cuhk.edu.hk/Intro-DH/projects

---
## Library resource: Data visualization
- Resource:
	- https://libguides.lib.cuhk.edu.hk/data_vis/home
- 4 major phases
	- Data extraction
	- Data Cleansing and Data Transformation
	- Data Analysis
	- Data Visualisation
- Tools
	- Planar visualization
	- Multi-dimensional visualization
	- Temporal visualization
	- Network visualization
	- Info-graphics / other types of visualization
- Software available at CUHK
	- refer to the page
- 

---


![[Pasted image 20230827165134.png|400]]
- Figure: the data science hierarchy of needs 

Big data challenges
- capturing data
- data storage
- search
- sharing
- transfer
- querying
- updating
- data source

Key skill sets
- Historical research skills
	- Content knowledge
	- Source analysis
	- Historiography
	- Contextualization
- Data analysis skills
	- Data literacy
	- Data collection
	- Data cleaning and preparation
	- Data aggregation: Combining data from various sources into a unified dataset for analysis
	- Data visualization: Creating compelling visual representations of historical data using tools like D3.js, Tableau, or Python libraries like Matplotlib and Seaborn.
	- Statistical analysis: Proficiency in using statistical tools and methods to uncover patterns and trends in historical data.
- Digital humanities skills
	- Text mining: Extracting insights from large volumes of historical texts through techniques like topic modeling, sentiment analysis, and named entity recognition.
	- Natural language processing (NLP): Applying NLP techniques to analyze and understand historical texts, including sentiment analysis, language translation, and text summarization.
	- Geo-spatial analysis
	- Network analysis
	- Digital archiving: Understanding principles of digital preservation, metadata, and creating digital archives.
- Programming and technical skills
	- Programming languages: Proficiency in programming languages commonly used in data analysis, such as Python or R.
	- Data manipulations: Using libraries and tools for data manipulation and analysis, such as pandas, NumPy, and R's tidyverse.
	- Database management: SQL and NoSQL: Understanding of structured and unstructured databases, including SQL databases like MySQL and NoSQL databases like MongoDB.
	- Web scraping: The ability to programmatically extract data from websites and online repositories relevant to historical research.
	- Version control: Using tools like Git for managing code and collaboration.
- Critical thinking
	- Interdisciplinary approach
	- Critical analysis
- communication skills
	- writing
	- presentation
	- collaboration 

Incorporating solutions provided by large language models (LLMs) like GPT.
- Problems:
	- Data Source Independence: You rightly point out that big data research often involves extracting and analyzing data from diverse and independent sources. If you were to use an LLM for data analysis, it might introduce biases or inaccuracies due to its training data, which could lead to incorrect interpretations of historical trends or events.
	- Data Freshness and Originality: Big data research aims to derive insights from the most recent and original data available. LLMs, on the other hand, are trained on a static dataset up until their knowledge cutoff date (in my case, September 2021). Therefore, they might not be as effective in analyzing truly fresh or updated data.
	- Understanding Historical Context: While LLMs can generate coherent text, they lack a deep understanding of historical context, nuances, and the complexities of global history. This deficiency could lead to misinterpretations or oversimplifications if LLMs are used in the analytical process.
	- Unintended Biases and Interpretations: LLMs can inadvertently generate biased or misleading information, which could skew the outcomes of your analysis. This is particularly concerning when dealing with sensitive historical topics or events.
	- Lack of Transparency: LLMs operate as "black boxes," making it challenging to understand how they arrive at specific conclusions. This lack of transparency could hinder your ability to validate or explain the results obtained from LLM-based analyses.
	- Complementing Human Expertise: Human experts possess domain knowledge, critical thinking skills, and an understanding of historical context that LLMs lack. Relying solely on LLMs could overlook the insights that human researchers can bring to the table.
- Usefulness:
	- Data Verification: As you mentioned, LLMs can be useful for cross-referencing or validating data obtained from other sources.
	- Text Summarization: LLMs can help in summarizing lengthy historical texts or documents, which can save time for human researchers.
	- Hypothesis Generation: LLMs can be employed to generate hypotheses or research questions based on the available data. However, these hypotheses should be thoroughly evaluated by human experts.
	- Language Translation: If your historical data involves multiple languages, LLMs can aid in translating and understanding texts.


Example problems
- Migration patterns analysis: Analyze data to understand trends, causes and consequences of population movements across regions and time periods
- Trade networks and cultural exchanges: Use trade and shipping data to map out historical trade routes, identifying key hubs and examining the cultural exchanges facilitated by these routes.
- Text mining historical documents: Apply natural language processing (NLP) techniques to analyze vast collections of historical texts, such as letters, diaries, and newspapers, to uncover societal sentiments, events, and trends.
- Visualizing Historical Time Series Data: Develop interactive visualizations to display historical data, making it easier to observe patterns, anomalies, and changes over time.
- Revolution and Social Movements Analysis: Study historical revolutionary and social movements using data to understand factors that led to their emergence, spread, and outcomes.

Example technologies
- Vector databases (example of them... ) (How to query them...)
- OpenAI embeddings

---

## Looking for -- what other universities teach "digital history"

- Stanford University - Spatial history - Spatial analysis
	- Was active in 2007 - 2022
	- More than 150 undergraduate students
	- http://web.stanford.edu/group/spatialhistory/static/

- Stanford University - Textual analysis
	- in 2010 the Literary Lab was established, focused on text mining of literary corpora. 

- Doing digital history - a beginner's guide to working with text as data
	- https://manchesteruniversitypress.co.uk/9781526132680/
	- Published date: May 2021

- Texus University - Programming for humanists
	- https://liberalarts.tamu.edu/codhr/programs/p4h/#:~:text=Programming%204%20Humanists%20(P4H)%20is,the%20Digital%20Humanities%20(DH).
	- Spring 2023: Beyond Encoding: Refining and Launching Text-Based Digital Humanities Projects
		- Topics to be covered include both project development (project management and sustainability, goal setting, content management, documentation, peer review, etc.) and technical skills to further develop XML-based projects (regex, git, schema development, XSLT, XML Databases, and XQuery).
	- Summer 2020 Gephi
		- generating network visualizations using Gephi, an open-source interactive platform, which allows users to highlight relationships between various elements. 
	- Spring 2019: TEI (Text encoding initiative)
		- TEI encoding, semantic markup, TEI schemas, principles of XSLT, XML transformation
---
## Techniques used in digital history

![[Pasted image 20230827202732.png]]
![[Pasted image 20230827203131.png]]
![[Pasted image 20230827201834.png]]
- Network analysis (gelfi, tableau)
- Graphical database, graphical query
- Six degrees of Francis Bacon (Load chancellor, politician, and philosopher)
	- Playground: http://www.sixdegreesoffrancisbacon.com/?ids=10000473&min_confidence=60&type=network


---
![[Pasted image 20230827204012.png]]
![[Pasted image 20230827204416.png]]
![[Pasted image 20230827204440.png]]

- Data in the humanities
	- "a state in which info is subjected to the strict codification of the measurable"
	- Data into archive, online resource, and computational analysis
	- "a further numerical, statistical, quantificatory activity upon the information is thereby made possible."

![[Pasted image 20230827204323.png]]
- Creating humanities computational models
	- McCarty - Humanities computing - 2005

![[Pasted image 20230827204920.png]]
- Methods and instruments of researches

![[Pasted image 20230827205101.png]]
![[Pasted image 20230827205206.png]]
![[Pasted image 20230827205240.png]]
- Data resources

![[Pasted image 20230827205702.png]]
![[Pasted image 20230827205740.png]]
- Text markup/markdown that preserve the information of an image

![[Pasted image 20230827205853.png]]
- Data encoding

![[Pasted image 20230827210142.png]]
![[Pasted image 20230827210259.png]]
![[Pasted image 20230827210427.png]]
- Text analysis / text mining / information retrieval

![[Pasted image 20230827210554.png]]
![[Pasted image 20230827210642.png]]
- Linked open data

---
- CUHK library: text-mining and analysis in digital scholarship research - tools of text mining
	- https://libguides.lib.cuhk.edu.hk/text-mining-analysis/tools
- Data collection
	- Auto-parsing tools: ParseHub, octoparse
	- Pythonic scraping tool: BS4, selenium
- Preprocessing
	- Corpro
	- Openrefine
	- CKIP tagger
- Feature extraction / modeling
	- KNIME (a visualization format data analysis)
	- Weka
	- Scikit-learn
	- Tensorflow (not much people use it now.)
	- PyTorch
- Visualization
	- Voyant
	- Tableau
	- Matplotlib
	- Seaborn

- figure
![[Pasted image 20230829201342.png|400]]



---
# Possible products after learning the course I

## Getting / accessing / mining data
- Social media data to uncover trends and conversations related to cultural and historical topics
- Knowing various data source - structured / unstructured / graphical data 
	- Linked open data
	- TEI-encoding, XML documents
- Querying REST API (like Twitter)
- Parsing XML documents, markdown documents (like TEI)

## Searching for data
- Filtering and querying from the raw data / database / server

## Creating labels / tags for data
- Tags / labels as "features" of data
- Simple data classifications with established machine learning pipelines
	- Such as classifying the type of data using LDA
- Preparing novel-length texts for textual analysis (clean-up, chapter segmenting, tokenization, lemmatization, etc.)
	- spaCy
		- Part-of-speech tagging
		- Named entity recognition
	- rule-baed spaCy
		- The entityRuler
		- The PhraseMatcher
		- Using RegEx with spaCy

## Data transformation and cleaning
- Language and translation tools - make data more available 
- Inspecting the status of data cleaning, in respect to the schema we propose. 
- BookNLP - A natural language processing pipeline for books
	- Output files
		- `.tokens`
		- `.entities`
		- `.quotes`
		- `.book`
	- Character analysis
	- Event analysis
## Database / data curation - Digital archives and repositories
- Host and curate collections of historical documents
- Related skill set:
	- Data processing
	- NLP / topic modeling / text analysis
	- Data querying

## Analysis from data
- Statistical analysis
- Text analysis: clustering / topic modeling
	- Bigrams and trigrams
	- LDA
	- Transformer model
	- Top2Vec 
- Sentiment analysis for literary studies
- word frequency analysis
- Social network analysis

## Solving domain-specific problem - Holocaust NER
## Publishing 
- Digital publishing / digital storytelling 
- Visualization / charting
- Creating simple but dynamic web applications for displaying data
- Designing an application with streamlit

- [[BookNLP]]

---
# Review: Statistical learning (the book)
- Some details of the tools can be referred to the book:
	- LDA for topic modeling
	- KNN for clustering
	- Neural network, Stochastic gradient descent, Document classification, RNN
- What humanists should avoid (at the time):
	- Avoid Using the results of generative AI as your data - It might have Hallucination on the facts that it doesn't know. 
	- 
---
# Some new tools in the recent times
- Python in excel (https://www.anaconda.com/excel)
- OpenAI embeddings
- Vector database