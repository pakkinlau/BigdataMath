---
category: []
alias: []
tags: []
---

- 30-10-2022 21:30: created

- What is Shannon information? (R1)
		- Shannon information = $log{1 \over p(x_h)}$ bits measures surprisal of each outcome of the set of possible outcomes. 
		- eg: flip coin, 90% heads up,  ^ef7e49
	- It often expressed as information = $-log\ p(x_h)$ bits
	- Take the average of all possible surprise / Shannon information, we have [[Cross entropy|entropy]]

![[Pasted image 20221030180725.png]]
- Fig: (R1)
---
## Reference

1. [[(Paper) Information Theory-- A Tutorial Introduction]]